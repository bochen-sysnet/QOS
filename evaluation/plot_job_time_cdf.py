#!/usr/bin/env python3
"""Plot CDFs for QPU seconds and queue/network overhead from saved real-job data.

Inputs:
- job cache JSONL generated by evaluation/full_eval.py
- optional full_eval logs (for elapsed_sec backfill on older cache entries)
"""

from __future__ import annotations

import argparse
import datetime as dt
import json
import math
import re
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple


_LOG_RE = re.compile(
    r"Real QPU job finished .* job_id=(?P<job_id>\S+) "
    r"elapsed_sec=(?P<elapsed>[0-9]*\.?[0-9]+) "
    r"qpu_sec=(?P<qpu>n/a|[0-9]*\.?[0-9]+)"
)


def _safe_float(v) -> Optional[float]:
    try:
        f = float(v)
    except Exception:
        return None
    if not math.isfinite(f):
        return None
    return f


def _load_job_cache(path: Path) -> List[dict]:
    rows: List[dict] = []
    if not path.exists():
        raise FileNotFoundError(f"Job cache file not found: {path}")
    with path.open("r", encoding="utf-8") as f:
        for i, line in enumerate(f, start=1):
            s = line.strip()
            if not s:
                continue
            try:
                rec = json.loads(s)
            except Exception:
                print(f"[warn] skip malformed JSONL line {i} in {path}")
                continue
            if isinstance(rec, dict):
                rows.append(rec)
    return rows


def _parse_logs(paths: Iterable[Path]) -> Dict[str, float]:
    elapsed_by_job: Dict[str, float] = {}
    for path in paths:
        if not path.exists():
            print(f"[warn] log file not found: {path}")
            continue
        with path.open("r", encoding="utf-8", errors="replace") as f:
            for line in f:
                m = _LOG_RE.search(line)
                if not m:
                    continue
                job_id = m.group("job_id")
                elapsed = _safe_float(m.group("elapsed"))
                if job_id and elapsed is not None:
                    elapsed_by_job[job_id] = elapsed
    return elapsed_by_job


def _ecdf(values: List[float]) -> Tuple[List[float], List[float]]:
    vals = sorted(values)
    n = len(vals)
    if n == 0:
        return [], []
    ys = [(i + 1) / n for i in range(n)]
    return vals, ys


def _filter_rows(
    rows: List[dict],
    backend: Optional[str],
    methods: Optional[set[str]],
    benches: Optional[set[str]],
    sizes: Optional[set[int]],
) -> List[dict]:
    out = []
    for r in rows:
        if backend and str(r.get("backend", "")) != backend:
            continue
        if methods and str(r.get("method", "")) not in methods:
            continue
        if benches and str(r.get("bench", "")) not in benches:
            continue
        if sizes:
            try:
                s = int(r.get("size"))
            except Exception:
                continue
            if s not in sizes:
                continue
        out.append(r)
    return out


def _plot(
    qpu_vals: List[float],
    queue_vals: List[float],
    out_path: Path,
    title_suffix: str,
) -> None:
    try:
        import matplotlib.pyplot as plt  # type: ignore
    except Exception:
        import matplotlib

        matplotlib.use("Agg")
        import matplotlib.pyplot as plt  # type: ignore

    fig, axes = plt.subplots(1, 2, figsize=(11.5, 4.4))

    x, y = _ecdf(qpu_vals)
    ax = axes[0]
    if x:
        ax.plot(x, y, linewidth=1.8)
        ax.set_title(f"QPU Time CDF{title_suffix}")
    else:
        ax.text(0.5, 0.5, "No qpu_sec data", ha="center", va="center", transform=ax.transAxes)
        ax.set_title("QPU Time CDF")
    ax.set_xlabel("qpu_sec")
    ax.set_ylabel("CDF")
    ax.grid(True, alpha=0.25)

    x, y = _ecdf(queue_vals)
    ax = axes[1]
    if x:
        ax.plot(x, y, linewidth=1.8)
        ax.set_title(f"Queue/Network CDF{title_suffix}")
    else:
        ax.text(
            0.5,
            0.5,
            "No elapsed_sec data.\nPass --logs for older caches.",
            ha="center",
            va="center",
            transform=ax.transAxes,
        )
        ax.set_title("Queue/Network CDF")
    ax.set_xlabel("elapsed_sec - qpu_sec")
    ax.set_ylabel("CDF")
    ax.grid(True, alpha=0.25)

    fig.tight_layout()
    out_path.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(out_path, bbox_inches="tight")
    plt.close(fig)


def _split_set(raw: str) -> Optional[set[str]]:
    vals = {x.strip() for x in raw.split(",") if x.strip()}
    return vals or None


def _split_int_set(raw: str) -> Optional[set[int]]:
    vals = set()
    for x in raw.split(","):
        x = x.strip()
        if not x:
            continue
        vals.add(int(x))
    return vals or None


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Plot CDFs of qpu_sec and queue/network overhead from full_eval job cache."
    )
    parser.add_argument(
        "--job-cache",
        default="evaluation/plots/full_eval_qpu_12_24_progress_job_cache.jsonl",
        help="Path to real-job cache JSONL.",
    )
    parser.add_argument(
        "--logs",
        default="",
        help="Optional comma-separated log files to backfill elapsed_sec for older cache entries.",
    )
    parser.add_argument("--backend", default="", help="Optional backend filter, e.g. ibm_torino.")
    parser.add_argument(
        "--methods",
        default="",
        help="Optional comma-separated method filter, e.g. Qiskit,QOS,CutQC,FrozenQubits.",
    )
    parser.add_argument(
        "--benches",
        default="",
        help="Optional comma-separated bench filter.",
    )
    parser.add_argument(
        "--sizes",
        default="",
        help="Optional comma-separated size filter, e.g. 12,24.",
    )
    parser.add_argument(
        "--out",
        default="",
        help="Output PDF path. Default: evaluation/plots/job_time_cdf_<timestamp>.pdf",
    )
    args = parser.parse_args()

    cache_path = Path(args.job_cache)
    rows = _load_job_cache(cache_path)
    methods = _split_set(args.methods)
    benches = _split_set(args.benches)
    sizes = _split_int_set(args.sizes) if args.sizes.strip() else None
    backend = args.backend.strip() or None

    rows = _filter_rows(rows, backend, methods, benches, sizes)
    print(f"[info] rows after filtering: {len(rows)}")

    elapsed_from_logs: Dict[str, float] = {}
    if args.logs.strip():
        log_paths = [Path(x.strip()) for x in args.logs.split(",") if x.strip()]
        elapsed_from_logs = _parse_logs(log_paths)
        print(f"[info] elapsed backfilled from logs: {len(elapsed_from_logs)} job ids")

    qpu_vals: List[float] = []
    queue_vals: List[float] = []
    elapsed_available = 0
    for r in rows:
        qpu = _safe_float(r.get("qpu_sec"))
        if qpu is not None:
            qpu_vals.append(qpu)

        elapsed = _safe_float(r.get("elapsed_sec"))
        if elapsed is None:
            job_id = str(r.get("job_id", ""))
            if job_id:
                elapsed = elapsed_from_logs.get(job_id)
        if elapsed is not None and qpu is not None:
            elapsed_available += 1
            queue_vals.append(max(0.0, elapsed - qpu))

    ts = dt.datetime.now().strftime("%Y%m%d_%H%M%S")
    out_path = Path(args.out) if args.out.strip() else Path("evaluation/plots") / f"job_time_cdf_{ts}.pdf"

    suffix_parts = []
    if backend:
        suffix_parts.append(backend)
    if methods:
        suffix_parts.append(f"methods={len(methods)}")
    if sizes:
        suffix_parts.append(f"sizes={','.join(str(x) for x in sorted(sizes))}")
    title_suffix = f" ({', '.join(suffix_parts)})" if suffix_parts else ""

    _plot(qpu_vals, queue_vals, out_path, title_suffix)
    print(f"[info] qpu samples: {len(qpu_vals)}")
    print(f"[info] elapsed samples: {elapsed_available}")
    print(f"[info] wrote figure: {out_path}")


if __name__ == "__main__":
    main()

